[
	{
		"id": "http://zotero.org/users/3423993/items/JPF2INM7",
		"type": "article-journal",
		"title": "Reasoning with Sarcasm by Reading In-between",
		"container-title": "arXiv:1805.02856 [cs]",
		"source": "arXiv.org",
		"abstract": "Sarcasm is a sophisticated speech act which commonly manifests on social communities such as Twitter and Reddit. The prevalence of sarcasm on the social web is highly disruptive to opinion mining systems due to not only its tendency of polarity flipping but also usage of figurative language. Sarcasm commonly manifests with a contrastive theme either between positive-negative sentiments or between literal-figurative scenarios. In this paper, we revisit the notion of modeling contrast in order to reason with sarcasm. More specifically, we propose an attention-based neural model that looks in-between instead of across, enabling it to explicitly model contrast and incongruity. We conduct extensive experiments on six benchmark datasets from Twitter, Reddit and the Internet Argument Corpus. Our proposed model not only achieves state-of-the-art performance on all datasets but also enjoys improved interpretability.",
		"URL": "http://arxiv.org/abs/1805.02856",
		"note": "00000 \narXiv: 1805.02856",
		"author": [
			{
				"family": "Tay",
				"given": "Yi"
			},
			{
				"family": "Tuan",
				"given": "Luu Anh"
			},
			{
				"family": "Hui",
				"given": "Siu Cheung"
			},
			{
				"family": "Su",
				"given": "Jian"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2018",
					5,
					8
				]
			]
		},
		"accessed": {
			"date-parts": [
				[
					"2018",
					5,
					9
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/3423993/items/ACF39ZAH",
		"type": "article-journal",
		"title": "One \"Ruler\" for All Languages: Multi-Lingual Dialogue Evaluation with Adversarial Multi-Task Learning",
		"container-title": "arXiv:1805.02914 [cs]",
		"source": "arXiv.org",
		"abstract": "Automatic evaluating the performance of Open-domain dialogue system is a challenging problem. Recent work in neural network-based metrics has shown promising opportunities for automatic dialogue evaluation. However, existing methods mainly focus on monolingual evaluation, in which the trained metric is not flexible enough to transfer across different languages. To address this issue, we propose an adversarial multi-task neural metric (ADVMT) for multi-lingual dialogue evaluation, with shared feature extraction across languages. We evaluate the proposed model in two different languages. Experiments show that the adversarial multi-task neural metric achieves a high correlation with human annotation, which yields better performance than monolingual ones and various existing metrics.",
		"URL": "http://arxiv.org/abs/1805.02914",
		"note": "00000 \narXiv: 1805.02914",
		"shortTitle": "One \"Ruler\" for All Languages",
		"author": [
			{
				"family": "Tong",
				"given": "Xiaowei"
			},
			{
				"family": "Fu",
				"given": "Zhenxin"
			},
			{
				"family": "Shang",
				"given": "Mingyue"
			},
			{
				"family": "Zhao",
				"given": "Dongyan"
			},
			{
				"family": "Yan",
				"given": "Rui"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2018",
					5,
					8
				]
			]
		},
		"accessed": {
			"date-parts": [
				[
					"2018",
					5,
					9
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/3423993/items/ABRRH7S8",
		"type": "article-journal",
		"title": "Hierarchical Structured Model for Fine-to-coarse Manifesto Text Analysis",
		"container-title": "arXiv:1805.02823 [cs]",
		"source": "arXiv.org",
		"abstract": "Election manifestos document the intentions, motives, and views of political parties. They are often used for analysing a party's fine-grained position on a particular issue, as well as for coarse-grained positioning of a party on the left--right spectrum. In this paper we propose a two-stage model for automatically performing both levels of analysis over manifestos. In the first step we employ a hierarchical multi-task structured deep model to predict fine- and coarse-grained positions, and in the second step we perform post-hoc calibration of coarse-grained positions using probabilistic soft logic. We empirically show that the proposed model outperforms state-of-art approaches at both granularities using manifestos from twelve countries, written in ten different languages.",
		"URL": "http://arxiv.org/abs/1805.02823",
		"note": "00000 \narXiv: 1805.02823",
		"author": [
			{
				"family": "Subramanian",
				"given": "Shivashankar"
			},
			{
				"family": "Cohn",
				"given": "Trevor"
			},
			{
				"family": "Baldwin",
				"given": "Timothy"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2018",
					5,
					8
				]
			]
		},
		"accessed": {
			"date-parts": [
				[
					"2018",
					5,
					9
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/3423993/items/I9LU668R",
		"type": "article-journal",
		"title": "Improving Character-level Japanese-Chinese Neural Machine Translation with Radicals as an Additional Input Feature",
		"container-title": "arXiv:1805.02937 [cs]",
		"source": "arXiv.org",
		"abstract": "In recent years, Neural Machine Translation (NMT) has been proven to get impressive results. While some additional linguistic features of input words improve word-level NMT, any additional character features have not been used to improve character-level NMT so far. In this paper, we show that the radicals of Chinese characters (or kanji), as a character feature information, can be easily provide further improvements in the character-level NMT. In experiments on WAT2016 Japanese-Chinese scientific paper excerpt corpus (ASPEC-JP), we find that the proposed method improves the translation quality according to two aspects: perplexity and BLEU. The character-level NMT with the radical input feature's model got a state-of-the-art result of 40.61 BLEU points in the test set, which is an improvement of about 8.6 BLEU points over the best system on the WAT2016 Japanese-to-Chinese translation subtask with ASPEC-JP. The improvements over the character-level NMT with no additional input feature are up to about 1.5 and 1.4 BLEU points in the development-test set and the test set of the corpus, respectively.",
		"URL": "http://arxiv.org/abs/1805.02937",
		"note": "00000 \narXiv: 1805.02937",
		"author": [
			{
				"family": "Zhang",
				"given": "Jinyi"
			},
			{
				"family": "Matsumoto",
				"given": "Tadahiro"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2018",
					5,
					8
				]
			]
		},
		"accessed": {
			"date-parts": [
				[
					"2018",
					5,
					9
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/3423993/items/ZALASN23",
		"type": "article-journal",
		"title": "Bleaching Text: Abstract Features for Cross-lingual Gender Prediction",
		"container-title": "arXiv:1805.03122 [cs]",
		"source": "arXiv.org",
		"abstract": "Gender prediction has typically focused on lexical and social network features, yielding good performance, but making systems highly language-, topic-, and platform-dependent. Cross-lingual embeddings circumvent some of these limitations, but capture gender-specific style less. We propose an alternative: bleaching text, i.e., transforming lexical strings into more abstract features. This study provides evidence that such features allow for better transfer across languages. Moreover, we present a first study on the ability of humans to perform cross-lingual gender prediction. We find that human predictive power proves similar to that of our bleached models, and both perform better than lexical models.",
		"URL": "http://arxiv.org/abs/1805.03122",
		"note": "00000 \narXiv: 1805.03122",
		"shortTitle": "Bleaching Text",
		"author": [
			{
				"family": "Goot",
				"given": "Rob",
				"non-dropping-particle": "van der"
			},
			{
				"family": "Ljubešić",
				"given": "Nikola"
			},
			{
				"family": "Matroos",
				"given": "Ian"
			},
			{
				"family": "Nissim",
				"given": "Malvina"
			},
			{
				"family": "Plank",
				"given": "Barbara"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2018",
					5,
					8
				]
			]
		},
		"accessed": {
			"date-parts": [
				[
					"2018",
					5,
					9
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/3423993/items/DB72SRPV",
		"type": "article-journal",
		"title": "Comparing phonemes and visemes with DNN-based lipreading",
		"container-title": "arXiv:1805.02924 [cs, eess]",
		"source": "arXiv.org",
		"abstract": "There is debate if phoneme or viseme units are the most effective for a lipreading system. Some studies use phoneme units even though phonemes describe unique short sounds; other studies tried to improve lipreading accuracy by focusing on visemes with varying results. We compare the performance of a lipreading system by modeling visual speech using either 13 viseme or 38 phoneme units. We report the accuracy of our system at both word and unit levels. The evaluation task is large vocabulary continuous speech using the TCD-TIMIT corpus. We complete our visual speech modeling via hybrid DNN-HMMs and our visual speech decoder is a Weighted Finite-State Transducer (WFST). We use DCT and Eigenlips as a representation of mouth ROI image. The phoneme lipreading system word accuracy outperforms the viseme based system word accuracy. However, the phoneme system achieved lower accuracy at the unit level which shows the importance of the dictionary for decoding classification outputs into words.",
		"URL": "http://arxiv.org/abs/1805.02924",
		"note": "00000 \narXiv: 1805.02924",
		"author": [
			{
				"family": "Thangthai",
				"given": "Kwanchiva"
			},
			{
				"family": "Bear",
				"given": "Helen L."
			},
			{
				"family": "Harvey",
				"given": "Richard"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2018",
					5,
					8
				]
			]
		},
		"accessed": {
			"date-parts": [
				[
					"2018",
					5,
					9
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/3423993/items/4CU3KG7P",
		"type": "article-journal",
		"title": "A Regression Model of Recurrent Deep Neural Networks for Noise Robust Estimation of the Fundamental Frequency Contour of Speech",
		"container-title": "arXiv:1805.02958 [cs, eess, stat]",
		"source": "arXiv.org",
		"abstract": "The fundamental frequency (F0) contour of speech is a key aspect to represent speech prosody that finds use in speech and spoken language analysis such as voice conversion and speech synthesis as well as speaker and language identification. This work proposes new methods to estimate the F0 contour of speech using deep neural networks (DNNs) and recurrent neural networks (RNNs). They are trained using supervised learning with the ground truth of F0 contours. The latest prior research addresses this problem first as a frame-by-frame-classification problem followed by sequence tracking using deep neural network hidden Markov model (DNN-HMM) hybrid architecture. This study, however, tackles the problem as a regression problem instead, in order to obtain F0 contours with higher frequency resolution from clean and noisy speech. Experiments using PTDB-TUG corpus contaminated with additive noise (NOISEX-92) show the proposed method improves gross pitch error (GPE) by more than 25 % at signal-to-noise ratios (SNRs) between -10 dB and +10 dB as compared with one of the most noise-robust F0 trackers, PEFAC. Furthermore, the performance on fine pitch error (FPE) is improved by approximately 20 % against a state-of-the-art DNN-HMM-based approach.",
		"URL": "http://arxiv.org/abs/1805.02958",
		"note": "00000 \narXiv: 1805.02958",
		"author": [
			{
				"family": "Kato",
				"given": "Akihiro"
			},
			{
				"family": "Kinnunen",
				"given": "Tomi"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2018",
					5,
					8
				]
			]
		},
		"accessed": {
			"date-parts": [
				[
					"2018",
					5,
					9
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/3423993/items/PW4L5W4D",
		"type": "article-journal",
		"title": "A Reinforcement Learning Approach to Interactive-Predictive Neural Machine Translation",
		"container-title": "arXiv:1805.01553 [cs, stat]",
		"source": "arXiv.org",
		"abstract": "We present an approach to interactive-predictive neural machine translation that attempts to reduce human effort from three directions: Firstly, instead of requiring humans to select, correct, or delete segments, we employ the idea of learning from human reinforcements in form of judgments on the quality of partial translations. Secondly, human effort is further reduced by using the entropy of word predictions as uncertainty criterion to trigger feedback requests. Lastly, online updates of the model parameters after every interaction allow the model to adapt quickly. We show in simulation experiments that reward signals on partial translations significantly improve character F-score and BLEU compared to feedback on full translations only, while human effort can be reduced to an average number of $5$ feedback requests for every input.",
		"URL": "http://arxiv.org/abs/1805.01553",
		"note": "00000 \narXiv: 1805.01553",
		"author": [
			{
				"family": "Lam",
				"given": "Tsz Kin"
			},
			{
				"family": "Kreutzer",
				"given": "Julia"
			},
			{
				"family": "Riezler",
				"given": "Stefan"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2018",
					5,
					3
				]
			]
		},
		"accessed": {
			"date-parts": [
				[
					"2018",
					5,
					9
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/3423993/items/QZ42EDJH",
		"type": "article-journal",
		"title": "Apply Chinese Radicals Into Neural Machine Translation: Deeper Than Character Level",
		"container-title": "arXiv:1805.01565 [cs]",
		"source": "arXiv.org",
		"abstract": "In neural machine translation (NMT), researchers face the challenge of un-seen (or out-of-vocabulary OOV) words translation. To solve this, some researchers propose the splitting of western languages such as English and German into sub-words or compounds. In this paper, we try to address this OOV issue and improve the NMT adequacy with a harder language Chinese whose characters are even more sophisticated in composition. We integrate the Chinese radicals into the NMT model with different settings to address the unseen words challenge in Chinese to English translation. On the other hand, this also can be considered as semantic part of the MT system since the Chinese radicals usually carry the essential meaning of the words they are constructed in. Meaningful radicals and new characters can be integrated into the NMT systems with our models. We use an attention-based NMT system as a strong baseline system. The experiments on standard Chinese-to-English NIST translation shared task data 2006 and 2008 show that our designed models outperform the baseline model in a wide range of state-of-the-art evaluation metrics including LEPOR, BEER, and CharacTER, in addition to the traditional BLEU and NIST scores, especially on the adequacy-level translation. We also have some interesting findings from the results of our various experiment settings about the performance of words and characters in Chinese NMT, which is different with other languages. For instance, the full character level NMT may perform very well or the state of the art in some other languages as researchers demonstrated recently, however, in the Chinese NMT model, word boundary knowledge is important for the model learning.",
		"URL": "http://arxiv.org/abs/1805.01565",
		"note": "arXiv: 1805.01565",
		"shortTitle": "Apply Chinese Radicals Into Neural Machine Translation",
		"author": [
			{
				"family": "Kuang",
				"given": "Shaohui"
			},
			{
				"family": "Han",
				"given": "Lifeng"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2018",
					5,
					3
				]
			]
		},
		"accessed": {
			"date-parts": [
				[
					"2018",
					5,
					9
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/3423993/items/AJFH3QGR",
		"type": "article-journal",
		"title": "Polite Dialogue Generation Without Parallel Data",
		"container-title": "arXiv:1805.03162 [cs]",
		"source": "arXiv.org",
		"abstract": "Stylistic dialogue response generation, with valuable applications in personality-based conversational agents, is a challenging task because the response needs to be fluent, contextually-relevant, as well as paralinguistically accurate. Moreover, parallel datasets for regular-to-stylistic pairs are usually unavailable. We present three weakly-supervised models that can generate diverse polite (or rude) dialogue responses without parallel data. Our late fusion model (Fusion) merges the decoder of an encoder-attention-decoder dialogue model with a language model trained on stand-alone polite utterances. Our label-fine-tuning (LFT) model prepends to each source sequence a politeness-score scaled label (predicted by our state-of-the-art politeness classifier) during training, and at test time is able to generate polite, neutral, and rude responses by simply scaling the label embedding by the corresponding score. Our reinforcement learning model (Polite-RL) encourages politeness generation by assigning rewards proportional to the politeness classifier score of the sampled response. We also present two retrieval-based polite dialogue model baselines. Human evaluation validates that while the Fusion and the retrieval-based models achieve politeness with poorer context-relevance, the LFT and Polite-RL models can produce significantly more polite responses without sacrificing dialogue quality.",
		"URL": "http://arxiv.org/abs/1805.03162",
		"note": "00000 \narXiv: 1805.03162",
		"author": [
			{
				"family": "Niu",
				"given": "Tong"
			},
			{
				"family": "Bansal",
				"given": "Mohit"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2018",
					5,
					8
				]
			]
		},
		"accessed": {
			"date-parts": [
				[
					"2018",
					5,
					9
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/3423993/items/Q2CXDB9I",
		"type": "article-journal",
		"title": "Interpretable Adversarial Perturbation in Input Embedding Space for Text",
		"container-title": "arXiv:1805.02917 [cs, stat]",
		"source": "arXiv.org",
		"abstract": "Following great success in the image processing field, the idea of adversarial training has been applied to tasks in the natural language processing (NLP) field. One promising approach directly applies adversarial training developed in the image processing field to the input word embedding space instead of the discrete input space of texts. However, this approach abandons such interpretability as generating adversarial texts to significantly improve the performance of NLP tasks. This paper restores interpretability to such methods by restricting the directions of perturbations toward the existing words in the input embedding space. As a result, we can straightforwardly reconstruct each input with perturbations to an actual text by considering the perturbations to be the replacement of words in the sentence while maintaining or even improving the task performance.",
		"URL": "http://arxiv.org/abs/1805.02917",
		"note": "00000 \narXiv: 1805.02917",
		"author": [
			{
				"family": "Sato",
				"given": "Motoki"
			},
			{
				"family": "Suzuki",
				"given": "Jun"
			},
			{
				"family": "Shindo",
				"given": "Hiroyuki"
			},
			{
				"family": "Matsumoto",
				"given": "Yuji"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2018",
					5,
					8
				]
			]
		},
		"accessed": {
			"date-parts": [
				[
					"2018",
					5,
					9
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/3423993/items/T6522WUA",
		"type": "article-journal",
		"title": "Mem2Seq: Effectively Incorporating Knowledge Bases into End-to-End Task-Oriented Dialog Systems",
		"container-title": "arXiv:1804.08217 [cs]",
		"source": "arXiv.org",
		"abstract": "End-to-end task-oriented dialog systems usually suffer from the challenge of incorporating knowledge bases. In this paper, we propose a novel yet simple end-to-end differentiable model called memory-to-sequence (Mem2Seq) to address this issue. Mem2Seq is the first neural generative model that combines the multi-hop attention over memories with the idea of pointer network. We empirically show how Mem2Seq controls each generation step, and how its multi-hop attention mechanism helps in learning correlations between memories. In addition, our model is quite general without complicated task-specific designs. As a result, we show that Mem2Seq can be trained faster and attain the state-of-the-art performance on three different task-oriented dialog datasets.",
		"URL": "http://arxiv.org/abs/1804.08217",
		"note": "00000 \narXiv: 1804.08217",
		"shortTitle": "Mem2Seq",
		"author": [
			{
				"family": "Madotto",
				"given": "Andrea"
			},
			{
				"family": "Wu",
				"given": "Chien-Sheng"
			},
			{
				"family": "Fung",
				"given": "Pascale"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2018",
					4,
					22
				]
			]
		},
		"accessed": {
			"date-parts": [
				[
					"2018",
					5,
					9
				]
			]
		}
	}
]